# ğŸ§  Automated Fact-Checking Prediction Using Machine Learning

![Pinocchio](./politifact.png)




## ğŸ” Project Overview

In an era of viral misinformation, this project aims to build an **automated fact-checking system** using machine learning. Leveraging labeled claims from [PolitiFact.com](https://www.politifact.com/), the model classifies public statements into six credibility levels:  
**True**, **Mostly True**, **Half True**, **Mostly False**, **False**, and **Pants on Fire**.

The final model was deployed using FastAPI and Google Cloud, offering a user-facing interface to assess the credibility of new claims.

ğŸŒ **Live App**: [Try the Fact-Checker â†’](https://fastapi-app-273008876300.us-central1.run.app/)

---




## ğŸ“„ Source Data

- **Original Labeled Statements**: Scraped from [PolitiFact.com](https://www.politifact.com/)
- **Scraping Notebook**: [`Copy_of_BeautifulSoupPractice.ipynb`](./Copy_of_BeautifulSoupPractice.ipynb)
  - [`politifact.csv`](./politifact.csv) â€” Structured dataset of labeled statements scraped from PolitiFact.com.  
  This file contains metadata including the statement text, speaker, rating, and publication date, used to train and evaluate fact-checking models.  
  > âš ï¸ *For educational and research purposes only. All content originally sourced from [PolitiFact.com](https://www.politifact.com/). Rights remain with the original publisher.*

- **Metadata Includes**:
  - Statement text
  - Speaker/source
  - Statement date
  - Labeled credibility (by human fact-checkers)

---

## ğŸ“š Notebooks

- [`Fact-Checking ML Model: Scaling Implementation.ipynb`](./Fact-Checking%20ML%20Model:%20Scaling%20Implementation.ipynb) â€” Data Wrangling & Visualization
- [`Evaluation_Report-3.ipynb`](./Evaluation_Report-3.ipynb) â€” Final Model Interpretation




_Notebooks not used in final deployment are stored in the `extras/` folder._

---

## ğŸ“‘ Documents

- ğŸ“˜ [Final Report (PDF)](./Automated%20Fact-Checking%20Prediction%20Using%20Machine%20Learning.pdf)

  Detailed explanation of methodology, challenges, and findings.

- ğŸ–¥ï¸ Slide Deck (coming soon)

---

## âš™ï¸ Model Overview

- Models Tested: Naive Bayes, Logistic Regression, SVM, XGBoost, Random Forest
- Best Model: **Logistic Regression** (Macro AUC = 0.7485)
- Key Features:
  - Cleaned and tokenized statements
  - One-hot encoded source categories
  - Temporal patterns from publication dates
- Interpretability:
  - Predictive word analysis using conditional probability with smoothing
  - Clear association between certain terms and credibility labels

---

## ğŸš€ Deployment

The final model is deployed on Google Cloud Run using FastAPI:

ğŸŒ [Access the Deployed App](https://fastapi-app-273008876300.us-central1.run.app/)

---

## ğŸ› ï¸ Future Work

- Automate real-time data scraping from PolitiFact
- Implement NER for better speaker/source classification
- Explore transformer-based architectures like BERT for deeper language understanding

---

## ğŸ“¸ Acknowledgments

- Data Source: [PolitiFact.com](https://www.politifact.com/)


---

